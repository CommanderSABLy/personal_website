{
    "what_is_gpt_quote": "ChatGPT is a large language model developed by OpenAI, designed to answer natural language questions and generate human-like responses. It uses deep learning techniques such as transformer-based neural networks to understand and generate text. ChatGPT has been trained on a massive corpus of text data and can generate responses on a wide range of topics, from general knowledge and current events to personal advice and creative writing prompts. As a language model, ChatGPT can be used for a variety of applications, including chatbots, question answering systems, and text generation tools.",
    "what_is_gpt": "ChatGPT is a chatbot that mimics human response. It has been trained on information from all throughout the internet. It has \"knowledge\" on the world up to 2019, including history, pop culture, internet trends, general knowledge, and computer science topics including programming. Because ChatGPT’s focus is on generating human-like responses, however, its accuracy is questionable. It always answers with utmost confidence but its answer can be anything from complete nonsense to entirely correct. For example, I asked ChatGPT about two Medal of Honor recipients: Ernest E. Evans and Vito Bertoldo. When asked who Ernest E. Evans was, it correctly stated he was a US Navy Medal of Honor Recipient that earned his medal for his actions in leading the USS Johnston in the Battle off Samar. It even got his birthday and hometown correct. On the other hand, it correctly stated that Vito Bertoldo was a Medal of Honor recipient, but got every other fact about him incorrect. Instead of being a rejected army volunteer from Illinois that single handedly held off German attacks in the Battle of the Bulge in World War Two as part of the 42nd Infantry division, ChatGPT claimed he was a man that led a patrol against a Chinese machine gun nest in the Korean War as part of the 42nd Division, born 16 years later than reality in Nevada and dying 44 years later than in reality.",
    "can_it_code": "Yes and no. It can cobble together some code from various repositories and documentations it has been trained on. That doesn't mean it is capable of creating original code or problem solving. You can ask it to solve questions it has seen before and it will sometimes produce a viable result. Take FizzBuzz for instance. This is a fairly simple problem with a very straightforward hardcoded solution. While not necessarily good practice, this kind of solution can answer the prompt. When prompted with \"write FizzBuzz in python\", ChatGPT provided the hardcoded solution.",
    "hardcoded": "for i in range(1, 101):\n\tif i % 3 == 0 and i % 5 == 0:\n\t\tprint(\"FizzBuzz\")\n\telif i % 3 == 0:\n\t\tprint(\"Fizz\")\n\telif i % 5 == 0:\n\t\tprint(\"Buzz\")\n\telse:\n\t\tprint(i)",
    "cic2": "A better solution will use variables for the modulo operation. This allows you to use any two integer values in the modulo operation. Another improvement would be making the solution a function that could be called elsewhere. Thirdly, it could use an output string to determine whether to output a word or a number. This allows you to check the values for \"Fizz\" and \"Buzz\" individually and add them to the string, negating the need for a separate check for both values simultaneously. Then, if the output string is not empty, print the string, otherwise, print the current value from the for-loop. When prompted \"now do it without hardcoding,\" it responded with all of these improvements.",
    "improved": "def fizzbuzz(start, end, fizz_factor, buzz_factor):\n\tfor i in range(start, end+1):\n\t\toutput = \"\"\n\t\tif i % fizz_factor == 0:\n\t\t\toutput += \"Fizz\"\n\t\tif i % buzz_factor == 0:\n\t\t\toutput += \"Buzz\"\n\t\tprint(output or i)",
    "cic3": "A simple modification to the FizzBuzz problem is changing the \"word\" to be printed for each value. As a function, this is a trivial change. Simply add a parameter for both \"words.\" To further improve the usability of the function, one could make defaults for the parameters so that a call without specifying the strings will run the basic \"Fizz\" and \"Buzz\" outputs. When given the prompt, \"Now write it with the ability to choose the output string for 'Fizz' and 'Buzz,'\" it returned the following function:",
    "improved_v2": "def fizzbuzz(start, end, fizz_factor, buzz_factor, fizz_string=\"Fizz\", buzz_string=\"Buzz\"):\n\tfor i in range(start, end+1):\n\t\toutput = \"\"\n\t\tif i % fizz_factor == 0:\n\t\t\toutput += fizz_string\n\t\tif i % buzz_factor == 0:\n\t\t\toutput += buzz_string\n\t\tprint(output or i)",
    "cic4": "This is a perfectly fine FizzBuzz function. Personally, I would not have included the start parameter unless prompted to do so and I would have made every parameter have a default such that running fizzbuzz() would result in the basic FizzBuzz solution being executed; numbers one through one hundred with multiples of three printing \"Fizz\" and multiples of five printing \"Buzz.\" Personal opinions aside, this is genuinely impressive. That this chatbot is capable of parsing the natural language prompts and produce functioning code that addresses the prompt is a major achievement. With that said, there are limitations to this technology.",
    "replace_dev": "No. ChatGPT and AI technology like it are not capable of replacing human developers or computer scientists in their current state and it is unlikely they will be capable of doing so for some time. The projects developers work on are too complex for the limitations of the technology in its current form and I do not see that problem going away any time soon. It is first and foremost a language model, not a programmer.",
    "why1": "ChatGPT has no understanding of computer science. It does not understand code, programming, or theory. It regurgitates information that it has been fed. It's like someone that crammed for a multiple choice history test. They retain trivia, pieces of information. They can even string together some of that information coherently sometimes. That does not mean they truly understand the topic, let alone make them a historian. History is largely about drawing connections. Having exact names and dates memorized is not all that important. What is important is being able to contextualize an event, a culture, a technology, or whatever specific subject one is studying. This same concept applies here, to AI language models like ChatGPT. It can spit out things it has seen, but it does not understand them. It can't create a new idea or innovate on what exists. It simply knows what order words or code typically appear in.",
    "why2": "This is abundantly clear with just two LeetCode prompts. ChatGPT’s training data goes up to 2021. When given LeetCode problem 9, Palindrome Number, ChatGPT provided the following code:",
    "palindrome": "def isPalindrome(x: int) -> bool:\n\tif x < 0:\n\t\treturn False\n\ts = str(x)\n\treturn s == s[::-1]",
    "why3": "LeetCode accepts this submission. Palindrome Number was listed as “easy,” so I gave ChatGPT the most recent “easy” problem as of writing this, LeetCode problem 2609, Find the Longest Balanced Substring of a Binary String. This was ChatGPT’s function:",
    "balancedSubstring": "def balancedSubstring(s: str) -> int:\n\tmax_len = 0\n\tzeros = ones = 0\n\tfor i in range(len(s)):\n\t\tif s[i] == '0':\n\t\t\tzeros += 1\n\t\telse:\n\t\t\tones += 1\n\t\tif zeros == ones:\n\t\t\tmax_len = max(max_len, zeros + ones)\n\treturn max_len",
    "why4": "This solution is not accepted. It fails the first of the three provided test cases and only passes 324 out of 1,848 test cases on submission. ChatGPT’s solution is simply counting ones and zeros. It is not actually checking substrings of s. What this shows us is that ChatGPT has an ability that any real person has: the ability to write a solution to a problem after having seen someone else’s solution. Not only does ChatGPT not understand computer science concepts, it does not have the ability to think critically. Even when informed that its solution did not work by providing it with the input, expected output, and actual output, it continued to produce incorrect solutions. Its first revision added a dictionary and kept track of the index for the value of the difference in number of zeros and ones. This gave the same result for the first test case as its first attempt. Every subsequent attempt used this same dictionary model, all with the same result. This lack of critical thinking, as well as an inability to retain context, can also be seen with a game of chess.",
    "why5": "I was inspired to play chess against ChatGPT after watching a video of someone pitting ChatGPT against itself. In the video, ChatGPT made illegal moves including taking its own pieces, adding a piece to the board that wasn’t there before, and castling twice. I decided to see what would happen if I told it that it could not perform its illegal move should it make one. The first eleven moves went smoothly, resulting in the following board state.",
    "why6": "At this point, it was ChatGPT’s turn as black. It proposed to move the knight at G8 to F6. Since it already had a pawn at F6, this was not a legal move. I was not surprised that ChatGPT would provide an illegal move, even this early into the match. It was not designed as a chess bot and has no way to visualize the board like a human does. What did surprise me was how many attempts it needed to come up with a legal move once it was told its move was illegal. I explained that the move was illegal because it already had a pawn at F6, yet its “corrected” response was the same move: knight at G8 to F6. I hit “regenerate response”, which resulted in an attempt to move the knight to E7, where its bishop was located. After another response regeneration, it attempted to move my pawn from E2 to E3, where I had a bishop. Another response regeneration circled back around to the first illegal move, G8 to F6. It took fifteen regenerations to respond with a legal move. Of those fifteen illegal moves, ten of them were the same illegal move that I had pointed out to it. This shows not just a lack of critical thinking, but a lack of thinking at all. Any polite player would point out that a move was illegal and allow their opponent to retry for a legal move. ChatGPT acknowledged that its move was illegal and tried to play it again, which no sane human would seriously attempt to do.",
    "why_quote": "As an AI language model, I am capable of understanding a wide range of topics, as long as they are presented to me in natural language. However, my understanding is based on patterns and associations in the data I was trained on, rather than true comprehension like a human being. While I can provide information and answers based on that training, I do not have true comprehension or consciousness.",
    "future": "ChatGPT can be a useful tool, especially for people that are trying to learn something. The current leading resource for programming help, other than one's peers, is StackOverflow. StackOverflow is notoriously unfriendly, especially for beginners. ChatGPT offers a judgement-free way to ask questions. While it may not be entirely reliable, it is likely to be significantly more reliable for a beginner's questions on account of the model being trained on documentation. This is great since beginners are the exact demographic least welcome on StackOverflow. Even for more adept programmers, this technology can be used as yet another tool in their metaphorical toolbox. No matter the amount of experience, a programmer will encounter problems they do not immediately know the solution to, and technology like ChatGPT will be another resource they can turn to, making development at least a little bit easier.",
    "video": "Cooper Codes produced a brief, excellent video on exactly this topic. He summarizes many of my points here in his two and a half minute video. Coder Foundry's video is a bit longer but also an excellent analysis on why AI will not be replacing developers any time soon. One of his points that I did not discuss here is the legality of AI produced code in products. I'm not qualified to speak on the topic, but it is an important aspect to the discussion to consider."
}